{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Exporting Keras models to LiteRT (TensorFlow Lite)\n",
    "\n",
    "**Author:** [Rahul Kumar](https://github.com/rahulkumar-aws)<br>\n",
    "**Date created:** 2025/12/10<br>\n",
    "**Last modified:** 2025/12/10<br>\n",
    "**Description:** Complete guide to exporting Keras models for mobile and edge deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "LiteRT (formerly TensorFlow Lite) enables you to deploy Keras models on mobile,\n",
    "embedded, and edge devices. This guide covers the **one-line export API** in Keras 3.x\n",
    "that makes mobile deployment simple.\n",
    "\n",
    "### What you'll learn\n",
    "\n",
    "- Export Keras models to `.tflite` format with a single line of code\n",
    "- Work with different model types (Sequential, Functional, Subclassed)\n",
    "- Export Keras-Hub pretrained models\n",
    "- Apply quantization for smaller model sizes\n",
    "- Handle dynamic input shapes\n",
    "\n",
    "### Key benefits\n",
    "\n",
    "- **One-line export**: `model.export(\"model.tflite\", format=\"litert\")`\n",
    "- **Multi-backend support**: Train with JAX/PyTorch, export to LiteRT\n",
    "- **Automatic input handling**: Works with dict inputs (Keras-Hub models)\n",
    "- **Built-in optimization**: Quantization support via `litert_kwargs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Quick Start: Export a Simple Model\n",
    "\n",
    "Let's start with a basic Sequential model and export it to LiteRT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=(784,)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Export to LiteRT (one line!)\n",
    "model.export(\"mnist_classifier.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported mnist_classifier.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "That's it! The model is now ready for mobile deployment.\n",
    "\n",
    "Let's verify the exported model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load and test the exported model\n",
    "try:\n",
    "    from ai_edge_litert.interpreter import Interpreter\n",
    "except ImportError:\n",
    "    from tensorflow.lite import Interpreter\n",
    "\n",
    "interpreter = Interpreter(model_path=\"mnist_classifier.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input/output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"\\nModel Input Details:\")\n",
    "print(f\"  Shape: {input_details[0]['shape']}\")\n",
    "print(f\"  Type: {input_details[0]['dtype']}\")\n",
    "\n",
    "print(\"\\nModel Output Details:\")\n",
    "print(f\"  Shape: {output_details[0]['shape']}\")\n",
    "print(f\"  Type: {output_details[0]['dtype']}\")\n",
    "\n",
    "# Test inference\n",
    "test_input = np.random.random((1, 784)).astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0][\"index\"], test_input)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "print(f\"\\nInference successful! Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exporting Different Model Types\n",
    "\n",
    "### Functional Models\n",
    "\n",
    "Functional models are the recommended way to build Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a Functional model\n",
    "inputs = keras.Input(shape=(224, 224, 3), name=\"image_input\")\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "functional_model = keras.Model(inputs=inputs, outputs=outputs, name=\"image_classifier\")\n",
    "\n",
    "# Export to LiteRT\n",
    "functional_model.export(\"image_classifier.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported Functional model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Subclassed Models\n",
    "\n",
    "For subclassed models, you must call the model with sample data first to establish\n",
    "input shapes before exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = layers.Dense(128, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(0.2)\n",
    "        self.dense2 = layers.Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.dense2(x)\n",
    "\n",
    "\n",
    "# Create and initialize the model\n",
    "custom_model = CustomModel()\n",
    "\n",
    "# IMPORTANT: Call model with sample data to establish input shapes\n",
    "sample_input = np.zeros((1, 784), dtype=np.float32)\n",
    "_ = custom_model(sample_input, training=False)\n",
    "\n",
    "# Now export\n",
    "custom_model.export(\"custom_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported Subclassed model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exporting Keras-Hub Models\n",
    "\n",
    "Keras-Hub models work seamlessly with the export API. Input configurations like\n",
    "sequence length are set during model construction via the preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras_hub\n",
    "\n",
    "# Load a pretrained text model\n",
    "# Sequence length is configured via the preprocessor\n",
    "preprocessor = keras_hub.models.GemmaCausalLMPreprocessor.from_preset(\n",
    "    \"gemma_2b_en\", sequence_length=128\n",
    ")\n",
    "\n",
    "model = keras_hub.models.GemmaCausalLM.from_preset(\n",
    "    \"gemma_2b_en\", preprocessor=preprocessor\n",
    ")\n",
    "\n",
    "# Export to LiteRT (sequence length already set)\n",
    "model.export(\"gemma_2b.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported Keras-Hub Gemma model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For vision models, the image size is determined by the preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load a vision model\n",
    "vision_model = keras_hub.models.ImageClassifier.from_preset(\n",
    "    \"efficientnetv2_b0_imagenet\"\n",
    ")\n",
    "\n",
    "# Export (image size already set by preset)\n",
    "vision_model.export(\"efficientnet.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported Keras-Hub vision model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Quantization for Smaller Models\n",
    "\n",
    "Quantization reduces model size and improves inference speed by using lower-precision\n",
    "data types. This is crucial for mobile deployment.\n",
    "\n",
    "### Dynamic Range Quantization\n",
    "\n",
    "The simplest quantization method converts weights from float32 to int8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "quantization_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=(784,)),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Export with dynamic range quantization\n",
    "quantization_model.export(\n",
    "    \"model_quantized.tflite\",\n",
    "    format=\"litert\",\n",
    "    litert_kwargs={\"optimizations\": [tf.lite.Optimize.DEFAULT]},\n",
    ")\n",
    "\n",
    "print(\"Exported quantized model\")\n",
    "\n",
    "# Compare file sizes\n",
    "import os\n",
    "\n",
    "original_size = os.path.getsize(\"mnist_classifier.tflite\") / 1024\n",
    "quantized_size = os.path.getsize(\"model_quantized.tflite\") / 1024\n",
    "\n",
    "print(f\"\\nOriginal model: {original_size:.2f} KB\")\n",
    "print(f\"Quantized model: {quantized_size:.2f} KB\")\n",
    "print(f\"Reduction: {(1 - quantized_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Float16 Quantization\n",
    "\n",
    "Float16 quantization provides a good balance between model size and accuracy,\n",
    "especially for GPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Export with float16 quantization\n",
    "quantization_model.export(\n",
    "    \"model_float16.tflite\",\n",
    "    format=\"litert\",\n",
    "    litert_kwargs={\n",
    "        \"optimizations\": [tf.lite.Optimize.DEFAULT],\n",
    "        \"target_spec\": {\"supported_types\": [tf.float16]},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Exported Float16 quantized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Full Integer Quantization (INT8)\n",
    "\n",
    "For maximum optimization, use full integer quantization with a representative dataset.\n",
    "This quantizes both weights and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare a representative dataset for calibration\n",
    "def representative_dataset():\n",
    "    \"\"\"Generate calibration data from your validation set.\"\"\"\n",
    "    for _ in range(100):\n",
    "        # Use real validation data for best results\n",
    "        sample = np.random.random((1, 784)).astype(np.float32)\n",
    "        yield [sample]\n",
    "\n",
    "\n",
    "# Export with INT8 quantization using TFLite converter directly\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quantization_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"model_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Exported INT8 quantized model\")\n",
    "\n",
    "int8_size = os.path.getsize(\"model_int8.tflite\") / 1024\n",
    "print(f\"INT8 model size: {int8_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dynamic Input Shapes\n",
    "\n",
    "Export models with flexible input dimensions that can be resized at runtime.\n",
    "This is useful for variable-length sequences or different image sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a model with dynamic sequence length\n",
    "dynamic_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(None,)),  # None = dynamic dimension\n",
    "        layers.Embedding(input_dim=1000, output_dim=64),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Export with dynamic shapes\n",
    "dynamic_model.export(\"dynamic_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Exported model with dynamic shapes\")\n",
    "\n",
    "# Verify dynamic shapes in the exported model\n",
    "interpreter = Interpreter(model_path=\"dynamic_model.tflite\")\n",
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "print(f\"\\nInput shape: {input_details[0]['shape']}\")\n",
    "print(\"Note: -1 indicates a dynamic dimension\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Custom Input Signatures\n",
    "\n",
    "For advanced use cases, you can specify custom input signatures using TensorSpec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a model expecting multiple inputs\n",
    "input1 = keras.Input(shape=(32,), name=\"input1\")\n",
    "input2 = keras.Input(shape=(64,), name=\"input2\")\n",
    "\n",
    "x1 = layers.Dense(64)(input1)\n",
    "x2 = layers.Dense(64)(input2)\n",
    "\n",
    "combined = layers.Concatenate()([x1, x2])\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(combined)\n",
    "\n",
    "multi_input_model = keras.Model(inputs=[input1, input2], outputs=outputs)\n",
    "\n",
    "# Export with custom input signature\n",
    "multi_input_model.export(\n",
    "    \"multi_input.tflite\",\n",
    "    format=\"litert\",\n",
    "    input_signature=[\n",
    "        tf.TensorSpec(shape=(None, 32), dtype=tf.float32, name=\"input1\"),\n",
    "        tf.TensorSpec(shape=(None, 64), dtype=tf.float32, name=\"input2\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Exported multi-input model with custom signature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Cross-Backend Export\n",
    "\n",
    "Keras 3.x supports multiple backends (JAX, PyTorch, TensorFlow). You can train\n",
    "with any backend and export to LiteRT using TensorFlow backend.\n",
    "\n",
    "### Training with JAX, Exporting with TensorFlow\n",
    "\n",
    "Here's a typical workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Simulate training with JAX backend (in a separate script)\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "# import keras\n",
    "# model = keras.Sequential([...])\n",
    "# model.fit(X_train, y_train)\n",
    "# model.save_weights(\"model_weights.weights.h5\")\n",
    "\n",
    "# Export script (use TensorFlow backend)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Recreate model architecture\n",
    "export_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation=\"relu\", input_shape=(784,)),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load weights trained with JAX (backend-agnostic weights)\n",
    "# export_model.load_weights(\"model_weights.weights.h5\")\n",
    "\n",
    "# Export to LiteRT\n",
    "export_model.export(\"cross_backend_model.tflite\", format=\"litert\")\n",
    "\n",
    "print(\"Cross-backend export demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Validation Best Practices\n",
    "\n",
    "Always verify your exported model before deploying to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_tflite_model(model_path, keras_model):\n",
    "    \"\"\"Compare TFLite model output with Keras model.\"\"\"\n",
    "    # Load TFLite model\n",
    "    interpreter = Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Create test input\n",
    "    test_input = np.random.random(input_details[0][\"shape\"]).astype(np.float32)\n",
    "\n",
    "    # Get Keras prediction\n",
    "    keras_output = keras_model.predict(test_input, verbose=0)\n",
    "\n",
    "    # Get TFLite prediction\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], test_input)\n",
    "    interpreter.invoke()\n",
    "    tflite_output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "    # Calculate difference\n",
    "    diff = np.abs(keras_output - tflite_output).mean()\n",
    "\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  Mean absolute difference: {diff:.6f}\")\n",
    "\n",
    "    if diff < 1e-4:\n",
    "        print(\"  Outputs match (excellent)\")\n",
    "    elif diff < 1e-2:\n",
    "        print(\"  Small difference (acceptable for quantized models)\")\n",
    "    else:\n",
    "        print(\"  Large difference (investigate!)\")\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "# Validate the exported model\n",
    "validate_tflite_model(\"mnist_classifier.tflite\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Troubleshooting Common Issues\n",
    "\n",
    "### Backend Error\n",
    "\n",
    "**Error**: `RuntimeError: Backend must be TensorFlow`\n",
    "\n",
    "**Solution**: Set the TensorFlow backend before importing Keras:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "```\n",
    "\n",
    "### Subclassed Model Export Fails\n",
    "\n",
    "**Error**: Unable to infer input signature for subclassed model\n",
    "\n",
    "**Solution**: Call the model with sample data before exporting:\n",
    "\n",
    "```python\n",
    "sample_input = np.zeros((1, 784), dtype=np.float32)\n",
    "_ = model(sample_input, training=False)\n",
    "model.export(\"model.tflite\", format=\"litert\")\n",
    "```\n",
    "\n",
    "### Unsupported Operations\n",
    "\n",
    "**Error**: Some ops are not supported by TFLite\n",
    "\n",
    "**Solution**: Enable TensorFlow ops in LiteRT:\n",
    "\n",
    "```python\n",
    "model.export(\n",
    "    \"model.tflite\",\n",
    "    format=\"litert\",\n",
    "    litert_kwargs={\n",
    "        \"target_spec\": {\n",
    "            \"supported_ops\": [\n",
    "                tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                tf.lite.OpsSet.SELECT_TF_OPS\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this guide, you learned how to:\n",
    "\n",
    "- Export Keras models to LiteRT format with `model.export()`\n",
    "- Work with Sequential, Functional, and Subclassed models\n",
    "- Export Keras-Hub pretrained models\n",
    "- Apply quantization to reduce model size (up to 75% reduction)\n",
    "- Handle dynamic input shapes for flexible inference\n",
    "- Validate exported models before deployment\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **One-line export**: `model.export(\"model.tflite\", format=\"litert\")`\n",
    "- **Use quantization**: Reduce size by 50-75% with minimal accuracy loss\n",
    "- **Always validate**: Test exported models before production deployment\n",
    "- **Backend flexibility**: Train with JAX/PyTorch, export with TensorFlow\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Deploy your `.tflite` model to mobile apps (Android/iOS)\n",
    "- Use GPU delegates for faster inference on mobile devices\n",
    "- Explore model optimization techniques for specific hardware\n",
    "- Check out the [Keras-Hub documentation](https://keras.io/keras_hub/) for\n",
    "  pretrained models\n",
    "\n",
    "For more details, see:\n",
    "- [Keras Serialization Guide](https://keras.io/guides/serialization_and_saving/)\n",
    "- [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)\n",
    "- [Quantization in Keras Guide](https://keras.io/guides/quantization_overview/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "C:\\projects\\Google\\keras-io\\guides\\litert_export",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}